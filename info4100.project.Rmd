---
title: 'Group Project: Early Alert with LMS Data'
author: '[[ADD YOUR NAME, CORNELL ID]]'
subtitle: INFO 4100 Learning Analytics
output:
  pdf_document: default
  word_document: default
  html_document:
    df_print: paged
---

```{r warning=FALSE, message=FALSE, echo=FALSE}
library(tidyverse)
# This loads 3 datasets: cl=clickstream, a=assessment grades; m=module states.
load("info4100_edx_2020-10-17.rda")
```

# Introduction

**Goals:** The goal of this project is to learn how to work with raw Learning Management System (LMS) data and apply some of the prediction skills you have learned so far. You will develop a one-day early warning system for students who miss a graded submission. I am sharing with you an export of the class's edX log data thus far. I have anonymized the dataset and performed minimal data cleaning, leaving plenty of real-world messiness for you to tackle here. As always, you should start by getting to know the datasets. In this case, you should be able to really understand what is going on because it is YOUR data. In fact, you can navigate to the relevant pages on edX to see what page/action the data refers to.

**Group Project:** This is a group project and I expect you to work as a team to come up with the best possible prediction accuracy. Your team will submit one common solution (note that EACH team member will need to submit the knitted Word doc on edx to get credit like with the first group project). 

**Try Your Best:** All members of the TWO teams that achieve the highest F1 scores will receive an extra credit point, and their solutions will be featured. To be eligible, your prediction problem needs to be set up correctly (i.e. everything else needs to be correct).

# Step 1: Understand the data

There are three datasets which can be connected using the hash_id column (a hashed version of the user id) and I am giving you links to the official documentation which you should read to understand the data better:

1. Clickstream data (1 row per student per action): [click for documentation](https://edx.readthedocs.io/projects/devdata/en/stable/internal_data_formats/tracking_logs.html#tracking-logs)
2. Module States (1 row per student per accessed content): original name [courseware-studentmodule (click for doumentation)](https://edx.readthedocs.io/projects/devdata/en/stable/internal_data_formats/sql_schema.html#courseware-studentmodule)
3. Assessment grades (1 row per assessment per student)

I have already converted date-time objects into a numeric `timestamp` for you.

To look up what pages URLs refer to (works for browser events, not server events), you can paste the URL into your browser. This should work for most URLs. I recommend doing this to be able to engineer more meaningful features.

*Question 1:* In the space below, explore each dataset using `head()`, `n_distinct(data$some_id)`, `summary()`, `table(data$column)`. You can also plot the distribution of variables with histograms or boxplots. Check out the data documentation linked above to understand the meaning of each column.

```{r}
############################################### 
###### BEGIN INPUT: Explore each dataset ###### 
###############################################

# Exploring Clickstreams
# add code here

# Exploring Module States
# add code here

# Exploring Assessment grades
# add code here

###############################################
###############################################
```

You may notice that it would be helpful to combine the information about grades and time of first attempt with the module state data. Below I make this join for you. See that only 'sequential' modules have grade data associated with them. The boxplot shows when the different sequentials (containing problems) were attempted. This gives you an idea of the order of problems in the course.

```{r}
ma = m %>% left_join(
    a %>% select(hash_id:possible_graded, first_attempted_timestamp), 
    by = c("hash_id"="hash_id", "module_id"="usage_key")
)

# Only sequential modules have a grade associated with them
table(ma$module_type, ma$first_attempted_timestamp>0)

# We see that assignments were due (submitted) at different times
boxplot(ma$first_attempted_timestamp ~ ma$module_id)
```

# Step 2: Define a prediction task

Recall the guidelines for defining a good prediction problem covered in the Handbook chapter on prediction. You are looking for something actionable (an opportunity to intervene) and a situation that repeats (so the prediction can be useful in the future). The tradeoff with the dataset you have here is that on the one hand it is very relevant to you but on the other hand it is relatively small. Still, the data is fine-grained and sufficiently messy to give you a taste of LMS data analysis.

The prediction problem for this project is to build a one-day early warning system for missing a graded submission. Specifically, **your goal is to predict one day before the submission deadline, if a student will forget to submit an assignment**, so that the system can send a reminder. As you may have noticed during the data exploration phase above (if not, you should go back and examine this), there are several graded submissions and some students missed one or more of them. We define **missing a submission** as having an NA for `first_attempted_timestamp` but of course only for those that are past due.

### Instructions

1. Treat each graded assignment as a prediction task (thus there are x*n prediction opportunities where x = number of graded assignments and n = 31 students).
2. Create a dataset that has 1 row per student per graded assessment with the binary outcome (did they MISS it? yes/no) and several predictors (see next tip)
3. Predictors (i.e. features) need to be engineered with data from **24hrs before each assignment is due**, which of course varies across assignments; that means you have much more information to predict later assignments than earlier ones
4. Once your dataset is ready, split it into a training and a test set
5. Train a prediction model on the training data; you can try out any of the ones we have covered in the prediction homework and Random Forest
6. Keep tuning your model choice, model parameters (if any), and feature engineering
6. Finally, test your prediction accuracy on the test set

# Step 3: Getting you started

## Create the outcome variable

**Identify the graded assessments and whether a student did NOT submit**. Recall we want to have a *warning* system, so the outcome should be the negative action.

Get the outcome for each graded assignment. Figure out the deadline for each and compute the timestamp for 24hrs prior to the deadline. You probably want to use the `ma` dataset I created for you above.

`r boxplot(ma$first_attempted_timestamp ~ ma$module_id)`

The following table helps you see the various graded assignments to consider. We keep only those where possible_graded > 0. **I define the deadline as the 90th percentile of submissions (you may use this simplification).**

```{r}
ma %>% 
    filter(possible_graded > 0) %>%
    group_by(module_id) %>% 
    summarise(
        deadline = quantile(first_attempted_timestamp, probs = .9, na.rm=T),
        p_unsubmitted = mean(is.na(first_attempted_timestamp))
    ) %>% 
    arrange(deadline)
```

Now you know which assessments (module_ids) to target. **Be sure to kick out the one with p_unsubmitted > 0.5**; They were not due yet when the export was created.

*Question 2:* Now build a dataset with an indicator for each person and each of these module_ids with 1=unsubmitted, 0=submitted. Keep track of the deadline: you only want to use features based on data up to 24hrs before it (i.e. `24 * 60 * 60` seconds).

```{r}
############################################### 
####### BEGIN INPUT: Define outcome ###########
###############################################

# add code here

############################################### 
############################################### 
```

## Feature Engineering

**For each graded assessment, identify what data is appropriate for feature engineering**

Before you start feature engineering, you need to constrain the data for **each** assessment.

Remember that the dataset we are aiming for has 1 row per person and assessment with several feature variables and one outcome variable. You created the outcome above. Now you need to create the appropriate features to join. I'm giving you an example for using `deadline = 1600304996` and creating 2 basic features from the clickstream. You should try to create a lot more features, including complex ones, that can use the clickstream or other datasets (but remember the timing constraint).

```{r}
secs_day = 60 * 60 * 24
example_deadline = 1600304996

example_features = cl %>% 
    filter(timestamp < example_deadline - secs_day) %>%
    group_by(hash_id) %>%
    summarise(
        num_events = n(),
        num_seq_goto = sum(event_type=="seq_goto")
    )

head(example_features)
```

*Question 3:* Engineer features for each student and assessment, subject to the timing constraint.

```{r}
############################################### 
###### BEGIN INPUT: Engineer features #########
###############################################

# add code here

###############################################
###############################################
```

# Step 4: Split your dataset

*Question 4:* We would like train the model on earlier assessments in order to make early alert predictions for later ones. As the hold-out test set, designate the four (4) last assessments (i.e. with the 4 latest computed deadlines, or the last 4 periods; same thing). You will use all the remaining data to train. Note that this may not be the best setup for all applications (e.g. if we wanted to use the model at the start of the course next year, but it is a reasonable approach if we wanted to use the model for the rest of this course offering). Identify the module_ids of the last four assignments, put data associated with their periods in the `test` dataset. Take all the remaining data (earlier periods excl the last 4) and put it in the `train` dataset.

```{r}
############################################### 
######## BEGIN INPUT: Split dataset ###########
###############################################

# Identify last 4 periods for testing
# add code here

# Split the dataset into train and test based on the module_ids or periods
# test = 
# train = 

###############################################
###############################################
```

# Step 5: Train your models

*Question 5:* Train a prediction model and iterate on it. You should try out different algorithms that you have learned so far. You can go back and check your features and refine them to get better performance. To check how well you are doing, you should focus on your training data and compute the F1 score: `F1 = 2/[(1/recall)+(1/precision)]`. Report your F1 score on the training data below (don't forget this!).

```{r}
############################################### 
####### BEGIN INPUT: Train and report #########
###############################################

# Fit  model to training data
# add code here

# Get predictions
# add code here

# Compute accuracy, recall, precision, and F1
# add code here
F1 = 2 / (1/recall + 1/precision)
  
# Training F1 score is ...
F1

###############################################
###############################################
```

# Step 6: Test your model

*Question 6:* Using the model that you arrived at, predict on the held-out test data and report your final F1 score. Typically, you would only do this once at the very end, but for this project it is actually rather hard to do well on the test set, so you can try your model (sparingly to avoid overfitting too much) on the test data to compute the testing F1 score.

```{r}
############################################### 
####### BEGIN INPUT: Test and report ##########
###############################################

# Make predictions on the test dataset
# add code here

# Compute F1
# add code here
F1 = 2 / (1/recall + 1/precision)

# Testing F1 score is ...
F1

###############################################
###############################################
```

# Step 7: Report

*Question 7:* As a team, write a brief report. Imagine your supervisor asked you to investigate the possibility of an early warning system. She would like to know what model to use, what features are important, and most importantly how well it would work. Given what you've learned, would you recommend implementing the system? Write your report answering the above questions here:

%######## BEGIN INPUT: Summarize findings ############

Add your summary here.

%###############################################

# Submit Project

This is the end of the project. Please **Knit a Word doc report** that shows both the R code and R output and upload it on the EdX platform. EACH TEAM MEMBER NEEDS TO SUBMIT THE REPORT ON EDX TO GET CREDIT.
