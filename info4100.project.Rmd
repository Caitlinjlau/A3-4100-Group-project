---
title: 'Group Project: Early Alert with LMS Data'
author: '[Caitlin Lau, CJL236]]'
subtitle: INFO 4100 Learning Analytics
output:
  pdf_document: default
  word_document: default
  html_document:
    df_print: paged
---

```{r warning=FALSE, message=FALSE, echo=FALSE}
library(tidyverse)
# This loads 3 datasets: cl=clickstream, a=assessment grades; m=module states.
load("info4100_edx_2020-10-17.rda")
```

# Introduction

**Goals:** The goal of this project is to learn how to work with raw Learning Management System (LMS) data and apply some of the prediction skills you have learned so far. You will develop a one-day early warning system for students who miss a graded submission. I am sharing with you an export of the class's edX log data thus far. I have anonymized the dataset and performed minimal data cleaning, leaving plenty of real-world messiness for you to tackle here. As always, you should start by getting to know the datasets. In this case, you should be able to really understand what is going on because it is YOUR data. In fact, you can navigate to the relevant pages on edX to see what page/action the data refers to.

**Group Project:** This is a group project and I expect you to work as a team to come up with the best possible prediction accuracy. Your team will submit one common solution (note that EACH team member will need to submit the knitted Word doc on edx to get credit like with the first group project). 

**Try Your Best:** All members of the TWO teams that achieve the highest F1 scores will receive an extra credit point, and their solutions will be featured. To be eligible, your prediction problem needs to be set up correctly (i.e. everything else needs to be correct).

# Step 1: Understand the data

There are three datasets which can be connected using the hash_id column (a hashed version of the user id) and I am giving you links to the official documentation which you should read to understand the data better:

1. Clickstream data (1 row per student per action): [click for documentation](https://edx.readthedocs.io/projects/devdata/en/stable/internal_data_formats/tracking_logs.html#tracking-logs)
2. Module States (1 row per student per accessed content): original name [courseware-studentmodule (click for doumentation)](https://edx.readthedocs.io/projects/devdata/en/stable/internal_data_formats/sql_schema.html#courseware-studentmodule)
3. Assessment grades (1 row per assessment per student)

I have already converted date-time objects into a numeric `timestamp` for you.

To look up what pages URLs refer to (works for browser events, not server events), you can paste the URL into your browser. This should work for most URLs. I recommend doing this to be able to engineer more meaningful features.

*Question 1:* In the space below, explore each dataset using `head()`, `n_distinct(data$some_id)`, `summary()`, `table(data$column)`. You can also plot the distribution of variables with histograms or boxplots. Check out the data documentation linked above to understand the meaning of each column.

```{r}
############################################### 
###### BEGIN INPUT: Explore each dataset ###### 
###############################################
#WE need to add more here
# Exploring Clickstreams
# add code here
head(cl)
# print("distinct hash id")
# n_distinct(cl$hash_id)
# n_distinct(cl$survey_id)
# n_distinct(cl$time)
# print("distinct name")
# n_distinct(cl$name)
# summary(cl)
# table(cl$event_source)

# Exploring Assessment grades
# add code here
head(a)
# print("distinct hash_id")
# n_distinct(a$hash_id)
# print("distinct created timestamp")
# n_distinct(a$created_timestamp)
# summary(a)
# table(a$created_timestamp)

# Exploring Module States
# add code here
head(m)
###############################################
###############################################
```

You may notice that it would be helpful to combine the information about grades and time of first attempt with the module state data. Below I make this join for you. See that only 'sequential' modules have grade data associated with them. The boxplot shows when the different sequentials (containing problems) were attempted. This gives you an idea of the order of problems in the course.

```{r}
ma = m %>% left_join(
    a %>% select(hash_id:possible_graded, first_attempted_timestamp), 
    by = c("hash_id"="hash_id", "module_id"="usage_key")
)

# Only sequential modules have a grade associated with them
table(ma$module_type, ma$first_attempted_timestamp>0)

# We see that assignments were due (submitted) at different times
boxplot(ma$first_attempted_timestamp ~ ma$module_id)

```

# Step 2: Define a prediction task

Recall the guidelines for defining a good prediction problem covered in the Handbook chapter on prediction. You are looking for something actionable (an opportunity to intervene) and a situation that repeats (so the prediction can be useful in the future). The tradeoff with the dataset you have here is that on the one hand it is very relevant to you but on the other hand it is relatively small. Still, the data is fine-grained and sufficiently messy to give you a taste of LMS data analysis.

The prediction problem for this project is to build a one-day early warning system for missing a graded submission. Specifically, **your goal is to predict one day before the submission deadline, if a student will forget to submit an assignment**, so that the system can send a reminder. As you may have noticed during the data exploration phase above (if not, you should go back and examine this), there are several graded submissions and some students missed one or more of them. We define **missing a submission** as having an NA for `first_attempted_timestamp` but of course only for those that are past due.

### Instructions

1. Treat each graded assignment as a prediction task (thus there are x*n prediction opportunities where x = number of graded assignments and n = 31 students).
2. Create a dataset that has 1 row per student per graded assessment with the binary outcome (did they MISS it? yes/no) and several predictors (see next tip)
3. Predictors (i.e. features) need to be engineered with data from **24hrs before each assignment is due**, which of course varies across assignments; that means you have much more information to predict later assignments than earlier ones
4. Once your dataset is ready, split it into a training and a test set
5. Train a prediction model on the training data; you can try out any of the ones we have covered in the prediction homework and Random Forest
6. Keep tuning your model choice, model parameters (if any), and feature engineering
6. Finally, test your prediction accuracy on the test set

# Step 3: Getting you started

## Create the outcome variable

**Identify the graded assessments and whether a student did NOT submit**. Recall we want to have a *warning* system, so the outcome should be the negative action.

Get the outcome for each graded assignment. Figure out the deadline for each and compute the timestamp for 24hrs prior to the deadline. You probably want to use the `ma` dataset I created for you above.

`r boxplot(ma$first_attempted_timestamp ~ ma$module_id)`

The following table helps you see the various graded assignments to consider. We keep only those where possible_graded > 0. **I define the deadline as the 90th percentile of submissions (you may use this simplification).**

```{r}
#We assigned table to something
unsubmit_tbl <- ma %>% 
    filter(possible_graded > 0) %>%
    group_by(module_id) %>% 
    summarise(
        deadline = quantile(first_attempted_timestamp, probs = .9, na.rm=T),
        p_unsubmitted = mean(is.na(first_attempted_timestamp))
    ) %>% 
    arrange(deadline)
unsubmit_tbl
#the assessment table has a lot of empty records in there even if no questions associated 
#p unsubmit looks up if most people have not submitted anything for it it is not a real assignment. And why it needs to be kicked out
#the ma table is missing the deadlines initially. 
#the ma table tries to figure out the deadline when ppl submit
```

Now you know which assessments (module_ids) to target. **Be sure to kick out the one with p_unsubmitted > 0.5**; They were not due yet when the export was created.

*Question 2:* Now build a dataset with an indicator for each person and each of these module_ids with 1=unsubmitted, 0=submitted. Keep track of the deadline: you only want to use features based on data up to 24hrs before it (i.e. `24 * 60 * 60` seconds).

```{r}
############################################### 
####### BEGIN INPUT: Define outcome ###########
###############################################
#should this be > 0.5
unsubmit_tbl = subset(unsubmit_tbl, p_unsubmitted <= 0.5)
new_unsubmit_tbl=merge(x = unsubmit_tbl, y = ma, by = "module_id", all.x = TRUE)
unsubmit_tbl
new_unsubmit_tbl

#subtract 24 hours from the deadline variable in order to find the day before ???
#1. need to build indicator if person has unsubmitted or submitted regardless of 24 hours use assesment table earned grade or first attempted field
#new column: if earned_grade=0 and first attempted timestamp in ma is NA then it is unsubmitted.
#new_unsubmit_tbl$is_submitted=(ifelse(is.na(new_unsubmit_tbl$first_attempted_timestamp), 1, 0 ))


# From Trevor: It looks like the is_submitted column is switched using the code above.  I'm going to rewrite the new below and leave / comment out the old.
new_unsubmit_tbl$is_submitted=(ifelse(is.na(new_unsubmit_tbl$first_attempted_timestamp), 0, 1 ))


# new column: the 24 hours thing is to figure out the last moment possible deadline-(24*60*60) to use features to predict if turned in or not. 
#new_unsubmit_tbl$is_submitted=(ifelse(new_unsubmit_tbl$deadline-(24*60*60)<new_unsubmit_tbl$first_attempted_timestamp, 1, 0))
new_unsubmit_tbl$deadline_minus_24 = (new_unsubmit_tbl$deadline-(60 * 60 * 24))
new_unsubmit_tbl


############################################### 
############################################### 
```

## Feature Engineering

**For each graded assessment, identify what data is appropriate for feature engineering**

Before you start feature engineering, you need to constrain the data for **each** assessment.

Remember that the dataset we are aiming for has 1 row per person and assessment with several feature variables and one outcome variable. You created the outcome above. Now you need to create the appropriate features to join. I'm giving you an example for using `deadline = 1600304996` and creating 2 basic features from the clickstream. You should try to create a lot more features, including complex ones, that can use the clickstream or other datasets (but remember the timing constraint).

```{r}
#just alwat remember to minus sec_day for example_deadline
secs_day = 60 * 60 * 24
example_deadline = 1600304996

example_features = cl %>% 
    filter(timestamp < example_deadline - secs_day) %>%
    group_by(hash_id) %>%
    summarise(
        num_events = n(),
        num_seq_goto = sum(event_type=="seq_goto")
    )

head(example_features)
```

*Question 3:* Engineer features for each student and assessment, subject to the timing constraint.

```{r}
############################################### 
###### BEGIN INPUT: Engineer features #########
###############################################
# cl
# a
# new_unsubmit_tbl
# cl group by name and filter out na for hash id. 
#5 pages that were top 5 search by pages (max frequency)
# top page click up until deadlien 24 mark 
#num clicks 
#usage_key links up with each module found in the a table.

#get more features and then reduce them on performance 
# feature 1: num load_video with a hash_ID 
# feature 2: num pause_video with a hash_ID
# feature 3: speed_change_video with hashID
# feature 4: stop_video with hashID
#feature 5: how many times they clicked on sllyabus (/courses/course-v1:Cornellx+INFO4100+Fall2020/courseware/2d815b2e787344838a1509c7a5861d2d/6fffcdccb3b84a8cbc79c173cbbe20e8/)
# feature 6: did they go to progress tab?
#/courses/course-v1:Cornellx+INFO4100+Fall2020/progress
#feature 7: seek_video
#feature 8: edx.grades.problem.submitted
#feature 9: 	edx.ui.lms.link_clicked
#feature 10: Did they click the slack tab
  #/courses/course-v1:Cornellx+INFO4100+Fall2020/f7c1480e105a4999839847b62fd7c19e/
#feature 11: num clicked on the week 2 reading
  #/courses/course-v1:Cornellx+INFO4100+Fall2020/courseware/6502c5233b274e2fa8797269fb79e890/8f630d1feb054691ac467f5c8c4bf68a/
#feature 12: num clicked on the week 5 reading
#feature 12: num clicked on the week 7 reading

#run a filter (NOT NECESSARRY)
#filter the people who clicked on the slyabus link (works as a search function to look up values you want NOT NECESARRY)
 cl_filter = cl %>%
  filter(name=="/courses/course-v1:Cornellx+INFO4100+Fall2020/courseware/6502c5233b274e2fa8797269fb79e890/8f630d1feb054691ac467f5c8c4bf68a/")
 cl_filter

#merge
#most features from cl table 
#a table max grade earned (only take grades that had happened prior to deadline)
#max grade earned (new_unsubmit table) make sure to check the <first submitted timestamp < deadline_minus_24
#merge by hash ID 
#new_unsubmit_tbl=merge(x = unsubmit_tbl, y = cl, by = "module_id", all.x = TRUE)

#merge hash id and deadline and new_unsubmit_table
cl_features = cl %>% 
    group_by(hash_id,) %>%
    summarise(
      num_load_video = sum(name=="load_video"),
      num_pause_video = sum(name=="pause_video"),
      num_speed_change_video = sum(name=="speed_change_video"),
      num_stop_video = sum(name=="stop_video"),
      num_click_syllabus = sum(name=="/courses/course-v1:Cornellx+INFO4100+Fall2020/courseware/2d815b2e787344838a1509c7a5861d2d/6fffcdccb3b84a8cbc79c173cbbe20e8/"),
      num_click_progress = sum(name=="/courses/course-v1:Cornellx+INFO4100+Fall2020/progress"),
      num_seek_video = sum(name=="seek_video"),
      num_name_prob_submit = sum(name=="edx.grades.problem.submitted"),
      num_slack_tab= sum(name=="/courses/course-v1:Cornellx+INFO4100+Fall2020/f7c1480e105a4999839847b62fd7c19e/"),
      num_week_two_reading= sum( name=="/courses/course-v1:Cornellx+INFO4100+Fall2020/courseware/6502c5233b274e2fa8797269fb79e890/8f630d1feb054691ac467f5c8c4bf68a/"),
      num_week_five_reading=sum(name=="/courses/course-v1:Cornellx+INFO4100+Fall2020/courseware/9c0661a25c0c4649a4971fb1a7d94c20/47161f5dde62497d9dd8f5b532de69d2/"),
      num_week_seven_reading= sum(name=="/courses/course-v1:Cornellx+INFO4100+Fall2020/courseware/7692a90992fe43acbe8caea218a2a595/bb26fa8b1f7648768c9ba9076f54e786/")
    )
cl_features
#new_unsubmit_tbl_features= new_unsubmit_tbl %>%
#  group_by(hash_id,) %>%
#  summarise()
  


#merge cl features and new_unsubmit_tbl_features by hash_ID 

# add code here
# textbook.pdf.page.scrolled
# event source=
###############################################
###############################################
```

# Step 4: Split your dataset

*Question 4:* We would like train the model on earlier assessments in order to make early alert predictions for later ones. As the hold-out test set, designate the four (4) last assessments (i.e. with the 4 latest computed deadlines, or the last 4 periods; same thing). You will use all the remaining data to train. Note that this may not be the best setup for all applications (e.g. if we wanted to use the model at the start of the course next year, but it is a reasonable approach if we wanted to use the model for the rest of this course offering). Identify the module_ids of the last four assignments, put data associated with their periods in the `test` dataset. Take all the remaining data (earlier periods excl the last 4) and put it in the `train` dataset.

```{r}
############################################### 
######## BEGIN INPUT: Split dataset ###########
###############################################

# Identify last 4 periods for testing

#list of modules ranked by deadline
first_modules = new_unsubmit_tbl %>% 
  group_by(module_id) %>%
  summarise(
    avg_deadline = mean(deadline_minus_24)
  ) %>%
  mutate(i = rank(avg_deadline))


#combining features and outcome
combined_hash = left_join(cl_features, new_unsubmit_tbl, by = 'hash_id')

#combining module_id rank based on deadline
combined_ranked = left_join(combined_hash, first_modules, by = 'module_id')


# Split the dataset into train and test based on the module_ids or periods

# test - last 4 modules
test = combined_ranked %>%
  filter(i >= 10) %>%
  group_by(hash_id) %>%
  subset(select = c(hash_id, num_load_video, num_pause_video, num_speed_change_video, num_stop_video,
                num_click_syllabus, num_click_progress, num_seek_video, num_name_prob_submit,
                num_slack_tab, num_week_two_reading, num_week_five_reading, num_week_seven_reading, is_submitted))


# train - first 9 modules
train = combined_ranked %>% 
  filter(i < 10) %>%
  subset(select = c(hash_id, num_load_video, num_pause_video, num_speed_change_video, num_stop_video,
                num_click_syllabus, num_click_progress, num_seek_video, num_name_prob_submit,
                num_slack_tab, num_week_two_reading, num_week_five_reading, num_week_seven_reading, is_submitted))

###############################################
###############################################
```

# Step 5: Train your models

*Question 5:* Train a prediction model and iterate on it. You should try out different algorithms that you have learned so far. You can go back and check your features and refine them to get better performance. To check how well you are doing, you should focus on your training data and compute the F1 score: `F1 = 2/[(1/recall)+(1/precision)]`. Report your F1 score on the training data below (don't forget this!).

```{r}
############################################### 
####### BEGIN INPUT: Train and report #########
###############################################

# Fit model to training data
# Linear Regression
m_linreg = lm(is_submitted ~ num_load_video + num_pause_video + num_speed_change_video + num_stop_video +
                num_click_syllabus + num_click_progress + num_seek_video + num_name_prob_submit +
                num_slack_tab + num_week_two_reading + num_week_five_reading + num_week_seven_reading
              , data = train)

# Logistic Regression

m_logreg = glm(is_submitted ~ num_load_video + num_pause_video + num_speed_change_video + num_stop_video +
                num_click_syllabus + num_click_progress + num_seek_video + num_name_prob_submit +
                num_slack_tab + num_week_two_reading + num_week_five_reading + num_week_seven_reading
              , data = train, family = 'binomial')


# Get predictions
# Linear Regression

p_linreg = predict(m_linreg, newdata = test)

p_linreg

# Logistic Regression

p_logreg = predict(m_logreg, newdata = test, type = 'response')
p_logreg = ifelse(p_logreg > .5, 1, 0)

p_logreg


# Compute accuracy, recall, precision, and F1
# add code here
F1 = 2 / (1/recall + 1/precision)
  
# Training F1 score is ...
F1

###############################################
###############################################
```

# Step 6: Test your model

*Question 6:* Using the model that you arrived at, predict on the held-out test data and report your final F1 score. Typically, you would only do this once at the very end, but for this project it is actually rather hard to do well on the test set, so you can try your model (sparingly to avoid overfitting too much) on the test data to compute the testing F1 score.

```{r}
############################################### 
####### BEGIN INPUT: Test and report ##########
###############################################

# Make predictions on the test dataset
# add code here

# Compute F1
# add code here
F1 = 2 / (1/recall + 1/precision)

# Testing F1 score is ...
F1

###############################################
###############################################
```

# Step 7: Report

*Question 7:* As a team, write a brief report. Imagine your supervisor asked you to investigate the possibility of an early warning system. She would like to know what model to use, what features are important, and most importantly how well it would work. Given what you've learned, would you recommend implementing the system? Write your report answering the above questions here:

%######## BEGIN INPUT: Summarize findings ############

Add your summary here.

%###############################################

# Submit Project

This is the end of the project. Please **Knit a Word doc report** that shows both the R code and R output and upload it on the EdX platform. EACH TEAM MEMBER NEEDS TO SUBMIT THE REPORT ON EDX TO GET CREDIT.
